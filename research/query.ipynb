{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e611d350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import uuid\n",
    "import torch\n",
    "import fitz\n",
    "import docx\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import json\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43512cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FOLDER = \"/home/logan78/projects/sih/database\" # Folder containing documents\n",
    "PERSIST_DIR = \"embeddings7/chromadb8\"  # Directory to persist ChromaDB\n",
    "CHUNKS_DIR = \"chunks\"  # Folder to store separate text chunks\n",
    "TEXT_MODEL_NAME = \"nomic-ai/nomic-embed-text-v1.5\" # Text embedding model\n",
    "IMAGE_MODEL_NAME = \"openai/clip-vit-large-patch14\"  # Image embedding\n",
    "MAX_WORDS_PER_CHUNK = 2000\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(CHUNKS_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8d70936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = SentenceTransformer(TEXT_MODEL_NAME, device=device, trust_remote_code=True)\n",
    "text_model.max_seq_length = 4096\n",
    "text_model.eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85bcb5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(image: Image.Image) -> np.ndarray:\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "    return features.squeeze().cpu().numpy()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=MAX_WORDS_PER_CHUNK,     \n",
    "    chunk_overlap=200                  \n",
    ")\n",
    "\n",
    "def split_text_to_chunks(text: str):\n",
    "    \"\"\"\n",
    "    Split text into chunks using LangChain RecursiveCharacterTextSplitter.\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "def split_text_to_chunks(text: str, max_words: int = MAX_WORDS_PER_CHUNK):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        chunk_words = words[start:start+max_words]\n",
    "        chunks.append(\" \".join(chunk_words))\n",
    "        start += max_words\n",
    "    return chunks\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str):\n",
    "    txts = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            txts.append(page.get_text(\"text\"))\n",
    "    return txts  # Return list of pages\n",
    "\n",
    "def extract_images_from_pdf(pdf_path: str):\n",
    "    images = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for i, page in enumerate(doc):\n",
    "            for img_index, img in enumerate(page.get_images(full=True)):\n",
    "                try:\n",
    "                    xref = img[0]\n",
    "                    base_image = doc.extract_image(xref)\n",
    "                    pil_image = Image.open(io.BytesIO(base_image[\"image\"])).convert(\"RGB\")\n",
    "                    image_id = f\"{os.path.basename(pdf_path)}_page_{i}_img_{img_index}\"\n",
    "                    images.append((i, pil_image, image_id))\n",
    "                except Exception as e:\n",
    "                    print(f\"[!] Error extracting image {img_index} on page {i}: {e}\")\n",
    "    return images\n",
    "\n",
    "def extract_text_from_docx(path: str):\n",
    "    doc = docx.Document(path)\n",
    "    return [p.text for p in doc.paragraphs if p.text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7846f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "collection_name = \"multimodal_embeddings\"\n",
    "\n",
    "if collection_name in [c.name for c in client.list_collections()]:\n",
    "    col = client.get_collection(collection_name)\n",
    "else:\n",
    "    col = client.create_collection(name=collection_name)\n",
    "\n",
    "def store_in_chroma(ids, embeddings, metadatas, documents):\n",
    "    col.add(ids=ids, embeddings=embeddings, metadatas=metadatas, documents=documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "952e3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def retrieve_from_chroma(query_text=None, query_image=None, top_k=10, mode=None):\n",
    "    \"\"\"\n",
    "    Retrieve top_k items from ChromaDB using multimodal query modes.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): Text query\n",
    "        query_image (str | PIL.Image): Image path or PIL.Image\n",
    "        top_k (int): Number of results to return\n",
    "        mode (str): Retrieval mode:\n",
    "            - \"text_to_text\": text using SentenceTransformer\n",
    "            - \"image_to_image\": image using CLIP image encoder\n",
    "            - \"text_to_image\": text using CLIP text encoder\n",
    "            - \"text_and_image_to_image\": combine CLIP text & image\n",
    "            - \"image_to_text\": image using CLIP image encoder (search text space)\n",
    "            If not provided, inferred automatically.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: Retrieved items with id, document, metadata, distance.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if not query_text and not query_image:\n",
    "        raise ValueError(\"Provide at least a text or image query.\")\n",
    "\n",
    "    if mode is None:\n",
    "        if query_text and not query_image:\n",
    "            mode = \"text_to_text\"\n",
    "        elif not query_text and query_image:\n",
    "            mode = \"image_to_image\"\n",
    "        elif query_text and query_image:\n",
    "            mode = \"text_and_image_to_image\"\n",
    "        else:\n",
    "            raise ValueError(\"Unable to infer retrieval mode.\")\n",
    "\n",
    "    query_emb = None\n",
    "\n",
    "    if mode == \"text_to_text\":\n",
    "        emb_text = text_model.encode(\n",
    "            query_text,\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True\n",
    "        ).cpu().numpy()\n",
    "        query_emb = emb_text.reshape(1, -1)\n",
    "\n",
    "\n",
    "    elif mode == \"image_to_image\":\n",
    "        if isinstance(query_image, str):\n",
    "            query_image = Image.open(query_image).convert(\"RGB\")\n",
    "        emb_image = embed_image(query_image)\n",
    "        query_emb = emb_image.reshape(1, -1)\n",
    "\n",
    "\n",
    "    elif mode == \"text_to_image\":\n",
    "        inputs = clip_processor(text=query_text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            emb_text_clip = clip_model.get_text_features(**inputs)\n",
    "            emb_text_clip = emb_text_clip / emb_text_clip.norm(dim=-1, keepdim=True)\n",
    "        query_emb = emb_text_clip.cpu().numpy().reshape(1, -1)\n",
    "\n",
    "\n",
    "    elif mode == \"text_and_image_to_image\":\n",
    "        if isinstance(query_image, str):\n",
    "            query_image = Image.open(query_image).convert(\"RGB\")\n",
    "\n",
    "      \n",
    "        inputs_text = clip_processor(text=query_text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            emb_text_clip = clip_model.get_text_features(**inputs_text)\n",
    "            emb_text_clip = emb_text_clip / emb_text_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "   \n",
    "        inputs_img = clip_processor(images=query_image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            emb_image_clip = clip_model.get_image_features(**inputs_img)\n",
    "            emb_image_clip = emb_image_clip / emb_image_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    \n",
    "        emb_combined = (emb_text_clip + emb_image_clip) / 2\n",
    "        emb_combined = emb_combined / emb_combined.norm(dim=-1, keepdim=True)\n",
    "        query_emb = emb_combined.cpu().numpy().reshape(1, -1)\n",
    "\n",
    "\n",
    "    elif mode == \"image_to_text\":\n",
    "        if isinstance(query_image, str):\n",
    "            query_image = Image.open(query_image).convert(\"RGB\")\n",
    "\n",
    "        inputs = clip_processor(images=query_image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            emb_image_clip = clip_model.get_image_features(**inputs)\n",
    "            emb_image_clip = emb_image_clip / emb_image_clip.norm(dim=-1, keepdim=True)\n",
    "        query_emb = emb_image_clip.cpu().numpy().reshape(1, -1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "  \n",
    "    results = col.query(\n",
    "        query_embeddings=query_emb.tolist(),\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "\n",
    "    retrieved = []\n",
    "    for idx, doc_id in enumerate(results[\"ids\"][0]):\n",
    "        retrieved.append({\n",
    "            \"id\": doc_id,\n",
    "            \"document\": results[\"documents\"][0][idx],\n",
    "            \"metadata\": results[\"metadatas\"][0][idx],\n",
    "            \"distance\": results[\"distances\"][0][idx],\n",
    "            \"retrieval_mode\": mode\n",
    "        })\n",
    "\n",
    "    return retrieved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb2c3284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: img-13-d261ece2-e64f-46d1-900b-27be3cb08c5d\n",
      "Document: [Image: NIPS-2017-attention-is-all-you-need-Paper.pdf_page_3_img_0]\n",
      "Metadata: {'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'type': 'image', 'page': 3, 'image_id': 'NIPS-2017-attention-is-all-you-need-Paper.pdf_page_3_img_0'}\n",
      "Distance: 1.5104589462280273\n",
      "--------------------------------------------------\n",
      "ID: img-12-21634407-6690-4a24-8569-d45e9e390dc8\n",
      "Document: [Image: NIPS-2017-attention-is-all-you-need-Paper.pdf_page_2_img_0]\n",
      "Metadata: {'page': 2, 'image_id': 'NIPS-2017-attention-is-all-you-need-Paper.pdf_page_2_img_0', 'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'type': 'image'}\n",
      "Distance: 1.5252233743667603\n",
      "--------------------------------------------------\n",
      "ID: img-14-c74099d6-738e-4e7c-935b-69261f337cd6\n",
      "Document: [Image: NIPS-2017-attention-is-all-you-need-Paper.pdf_page_3_img_1]\n",
      "Metadata: {'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'type': 'image', 'page': 3, 'image_id': 'NIPS-2017-attention-is-all-you-need-Paper.pdf_page_3_img_1'}\n",
      "Distance: 1.6172789335250854\n",
      "--------------------------------------------------\n",
      "ID: img-0-8025f59f-9fd5-4ada-84ba-475a53b919c6\n",
      "Document: [Image: image.png]\n",
      "Metadata: {'page': -1, 'image_id': '', 'filename': 'image.png', 'path': '/home/logan78/projects/sih/database/image.png', 'type': 'image'}\n",
      "Distance: 1.6730998754501343\n",
      "--------------------------------------------------\n",
      "ID: txt-10-f5ed822f-6964-41ea-8cd6-013dc9aca7b9\n",
      "Document: References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017. [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016. [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016. [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. [8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017. [9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016. [11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies, 2001. [12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. [13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016. [14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016. [15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017. [16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017. [17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017. [19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017. [20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016. 10\n",
      "Metadata: {'page': 9, 'type': 'text', 'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'image_id': ''}\n",
      "Distance: 1.9596682786941528\n",
      "--------------------------------------------------\n",
      "ID: txt-1-e686a200-a94b-4fae-9a19-f91df0d48377\n",
      "Document: Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗† University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. 1 Introduction Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13]. ∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. †Work performed while at Google Brain. ‡Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "Metadata: {'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 0, 'image_id': '', 'type': 'text', 'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf'}\n",
      "Distance: 1.9624403715133667\n",
      "--------------------------------------------------\n",
      "ID: txt-3-28ea3bf6-5a6f-457e-82c4-8f09b386066a\n",
      "Document: Figure 1: The Transformer - model architecture. wise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the 3\n",
      "Metadata: {'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'type': 'text', 'image_id': '', 'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 2}\n",
      "Distance: 1.9644653797149658\n",
      "--------------------------------------------------\n",
      "ID: txt-2-b32fcae0-8201-4e92-a04b-27ce74be27c9\n",
      "Document: Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2\n",
      "Metadata: {'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 1, 'image_id': '', 'type': 'text', 'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf'}\n",
      "Distance: 1.9715827703475952\n",
      "--------------------------------------------------\n",
      "ID: txt-6-dc5df421-2b0a-4f4f-ae2b-0af4fd03f553\n",
      "Document: Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2 · d) O(1) O(1) Recurrent O(n · d2) O(n) O(n) Convolutional O(k · n · d2) O(1) O(logk(n)) Self-Attention (restricted) O(r · n · d) O(1) O(n/r) bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and ﬁxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel) where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of PEpos. We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in 6\n",
      "Metadata: {'image_id': '', 'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 5, 'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'type': 'text'}\n",
      "Distance: 1.9774901866912842\n",
      "--------------------------------------------------\n",
      "ID: txt-9-42fceb91-760b-4c6b-bbc6-3202d07e526e\n",
      "Document: Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. N dmodel dff h dk dv Pdrop ϵls train PPL BLEU params steps (dev) (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A) 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B) 16 5.16 25.1 58 32 5.01 25.4 60 (C) 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D) 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor. Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration. 9\n",
      "Metadata: {'type': 'text', 'page': 8, 'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'image_id': '', 'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf'}\n",
      "Distance: 1.990596890449524\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = retrieve_from_chroma(query_text = \"tell me about attention mechanism and how it works can you show figure and diagam \" ,top_k=10,mode=\"text_to_image\")\n",
    "\n",
    "for r in results:\n",
    "    print(\"ID:\", r['id'])\n",
    "    print(\"Document:\", r['document'])\n",
    "    print(\"Metadata:\", r['metadata'])\n",
    "    print(\"Distance:\", r['distance'])\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0dacef36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'img-13-d261ece2-e64f-46d1-900b-27be3cb08c5d',\n",
       "  'document': '[Image: NIPS-2017-attention-is-all-you-need-Paper.pdf_page_3_img_0]',\n",
       "  'metadata': {'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'type': 'image',\n",
       "   'page': 3,\n",
       "   'image_id': 'NIPS-2017-attention-is-all-you-need-Paper.pdf_page_3_img_0'},\n",
       "  'distance': 1.5104589462280273,\n",
       "  'retrieval_mode': 'text_to_image'},\n",
       " {'id': 'img-12-21634407-6690-4a24-8569-d45e9e390dc8',\n",
       "  'document': '[Image: NIPS-2017-attention-is-all-you-need-Paper.pdf_page_2_img_0]',\n",
       "  'metadata': {'page': 2,\n",
       "   'image_id': 'NIPS-2017-attention-is-all-you-need-Paper.pdf_page_2_img_0',\n",
       "   'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'type': 'image'},\n",
       "  'distance': 1.5252233743667603,\n",
       "  'retrieval_mode': 'text_to_image'},\n",
       " {'id': 'img-14-c74099d6-738e-4e7c-935b-69261f337cd6',\n",
       "  'document': '[Image: NIPS-2017-attention-is-all-you-need-Paper.pdf_page_3_img_1]',\n",
       "  'metadata': {'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'type': 'image',\n",
       "   'page': 3,\n",
       "   'image_id': 'NIPS-2017-attention-is-all-you-need-Paper.pdf_page_3_img_1'},\n",
       "  'distance': 1.6172789335250854,\n",
       "  'retrieval_mode': 'text_to_image'},\n",
       " {'id': 'img-0-8025f59f-9fd5-4ada-84ba-475a53b919c6',\n",
       "  'document': '[Image: image.png]',\n",
       "  'metadata': {'page': -1,\n",
       "   'image_id': '',\n",
       "   'filename': 'image.png',\n",
       "   'path': '/home/logan78/projects/sih/database/image.png',\n",
       "   'type': 'image'},\n",
       "  'distance': 1.6730998754501343,\n",
       "  'retrieval_mode': 'text_to_image'},\n",
       " {'id': 'txt-10-f5ed822f-6964-41ea-8cd6-013dc9aca7b9',\n",
       "  'document': 'References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017. [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016. [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016. [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. [8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017. [9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016. [11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies, 2001. [12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. [13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016. [14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016. [15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017. [16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017. [17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017. [19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017. [20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016. 10',\n",
       "  'metadata': {'page': 9,\n",
       "   'type': 'text',\n",
       "   'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'image_id': ''},\n",
       "  'distance': 1.9596682786941528,\n",
       "  'retrieval_mode': 'text_to_image'},\n",
       " {'id': 'txt-1-e686a200-a94b-4fae-9a19-f91df0d48377',\n",
       "  'document': 'Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗† University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. 1 Introduction Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13]. ∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. †Work performed while at Google Brain. ‡Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.',\n",
       "  'metadata': {'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'page': 0,\n",
       "   'image_id': '',\n",
       "   'type': 'text',\n",
       "   'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf'},\n",
       "  'distance': 1.9624403715133667,\n",
       "  'retrieval_mode': 'text_to_image'},\n",
       " {'id': 'txt-3-28ea3bf6-5a6f-457e-82c4-8f09b386066a',\n",
       "  'document': 'Figure 1: The Transformer - model architecture. wise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the 3',\n",
       "  'metadata': {'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'type': 'text',\n",
       "   'image_id': '',\n",
       "   'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'page': 2},\n",
       "  'distance': 1.9644653797149658,\n",
       "  'retrieval_mode': 'text_to_image'},\n",
       " {'id': 'txt-2-b32fcae0-8201-4e92-a04b-27ce74be27c9',\n",
       "  'document': 'Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2',\n",
       "  'metadata': {'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'page': 1,\n",
       "   'image_id': '',\n",
       "   'type': 'text',\n",
       "   'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf'},\n",
       "  'distance': 1.9715827703475952,\n",
       "  'retrieval_mode': 'text_to_image'},\n",
       " {'id': 'txt-6-dc5df421-2b0a-4f4f-ae2b-0af4fd03f553',\n",
       "  'document': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2 · d) O(1) O(1) Recurrent O(n · d2) O(n) O(n) Convolutional O(k · n · d2) O(1) O(logk(n)) Self-Attention (restricted) O(r · n · d) O(1) O(n/r) bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and ﬁxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel) where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of PEpos. We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in 6',\n",
       "  'metadata': {'image_id': '',\n",
       "   'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'page': 5,\n",
       "   'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'type': 'text'},\n",
       "  'distance': 1.9774901866912842,\n",
       "  'retrieval_mode': 'text_to_image'},\n",
       " {'id': 'txt-9-42fceb91-760b-4c6b-bbc6-3202d07e526e',\n",
       "  'document': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. N dmodel dff h dk dv Pdrop ϵls train PPL BLEU params steps (dev) (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A) 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B) 16 5.16 25.1 58 32 5.01 25.4 60 (C) 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D) 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor. Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration. 9',\n",
       "  'metadata': {'type': 'text',\n",
       "   'page': 8,\n",
       "   'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "   'image_id': '',\n",
       "   'path': '/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need-Paper.pdf'},\n",
       "  'distance': 1.990596890449524,\n",
       "  'retrieval_mode': 'text_to_image'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3fbc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyC2C2huU3oN-LQEvIZZCNff4vQhWYtXU5Q\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16b0ce35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"topic_name\": \"Document Subject Area Pie Chart\",\n",
      "  \"response_text\": \"The retrieved image at /home/logan78/projects/sih/database/image.png likely contains a pie chart or other diagram related to the subject area of the document. Without further processing of the image, I cannot provide specific details about the pie chart's contents. The text document, 'NIPS-2017-attention-is-all-you-need.pdf' on page 3, discusses the Transformer model and self-attention, which is not directly related to the pie chart or the overall subject distribution of a document.\",\n",
      "  \"retrieved_items\": [\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"path\": \"/home/logan78/projects/sih/database/image.png\",\n",
      "      \"page_number\": -1,\n",
      "      \"content_summary\": \"Image at /home/logan78/projects/sih/database/image.png (CLIP embeddings processed). Likely contains diagrams or visual info.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, WhisperProcessor, WhisperForConditionalGeneration\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# -----------------------\n",
    "# LLM Setup\n",
    "# -----------------------\n",
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a powerful multimodal reasoning AI.\n",
    "You are given:\n",
    "1. A user query.\n",
    "2. Retrieved context (text, images, or audio) with metadata.\n",
    "\n",
    "Task:\n",
    "- Only use relevant items to answer the query.\n",
    "- For images or audio, summarize the content.\n",
    "- Return ONLY JSON with fields:\n",
    "{{\n",
    "  \"topic_name\": \"<key topic>\",\n",
    "  \"response_text\": \"<detailed answer>\",\n",
    "  \"retrieved_items\": [\n",
    "      {{\n",
    "          \"type\": \"<text|image|audio>\",\n",
    "          \"path\": \"<path>\",\n",
    "          \"page_number\": <page_number or -1>,\n",
    "          \"content_summary\": \"<summary>\"\n",
    "      }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Query: {query}\n",
    "Retrieved Items:\n",
    "{retrieved_items}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# -----------------------\n",
    "# Device Setup\n",
    "# -----------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def summarize_image(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_embeds = clip_model.get_image_features(**inputs)\n",
    "        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return f\"Image at {path} (CLIP embeddings processed). Likely contains diagrams or visual info.\"\n",
    "\n",
    "# -----------------------\n",
    "# Whisper for Audio Transcription\n",
    "# -----------------------\n",
    "whisper_model_name = \"openai/whisper-small\"\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name).to(device)\n",
    "whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
    "\n",
    "def summarize_audio(path):\n",
    "    import torchaudio\n",
    "    speech_array, sr = torchaudio.load(path)\n",
    "    inputs = whisper_processor(speech_array, sampling_rate=sr, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = whisper_model.generate(inputs.input_features)\n",
    "    transcription = whisper_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return f\"Audio at {path}: {transcription}\"\n",
    "\n",
    "# -----------------------\n",
    "# Prepare Retrieved Items\n",
    "# -----------------------\n",
    "def prepare_retrieved_items(items):\n",
    "    prepared = []\n",
    "    for item in items:\n",
    "        metadata = item.get(\"metadata\", {})\n",
    "        item_type = metadata.get(\"type\", \"text\")\n",
    "        path = metadata.get(\"path\", \"\")\n",
    "        page = metadata.get(\"page\", -1)\n",
    "\n",
    "        if item_type == \"text\":\n",
    "            content_summary = item.get(\"document\", \"\")\n",
    "        elif item_type == \"image\":\n",
    "            content_summary = summarize_image(path)\n",
    "        elif item_type == \"audio\":\n",
    "            content_summary = summarize_audio(path)\n",
    "        else:\n",
    "            content_summary = \"[Unknown type]\"\n",
    "\n",
    "        prepared.append({\n",
    "            \"type\": item_type,\n",
    "            \"path\": path,\n",
    "            \"page_number\": page,\n",
    "            \"content_summary\": content_summary\n",
    "        })\n",
    "    return json.dumps(prepared, indent=2)\n",
    "\n",
    "# -----------------------\n",
    "# Main Multimodal Agent\n",
    "# -----------------------\n",
    "def multimodal_agent(query, retrieved_items):\n",
    "    retrieved_json = prepare_retrieved_items(retrieved_items)\n",
    "    response = chain.invoke({\n",
    "        \"query\": query,\n",
    "        \"retrieved_items\": retrieved_json\n",
    "    })\n",
    "    return response\n",
    "\n",
    "# -----------------------\n",
    "# Example Usage\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"give me pie chart about document by subject area\"\n",
    "    retrieved_items = [\n",
    "        {\n",
    "            \"document\": \"[Image: image.png]\",\n",
    "            \"metadata\": {\n",
    "                \"path\": \"/home/logan78/projects/sih/database/image.png\",\n",
    "                \"page\": -1,\n",
    "                \"type\": \"image\"\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"document\": \"Transformer uses self-attention for sequence modeling...\",\n",
    "            \"metadata\": {\n",
    "                \"path\": \"/home/logan78/projects/sih/database/NIPS-2017-attention-is-all-you-need.pdf\",\n",
    "                \"page\": 3,\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    result = multimodal_agent(query, retrieved_items)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b75dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
